from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.settings import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.openai import OpenAI
import faiss
import os
import openai

openai.api_key = "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"

Settings.llm = None  # Kh√¥ng s·ª≠ d·ª•ng LLM trong qu√° tr√¨nh t·∫°o index

# 1. Load d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c vƒÉn b·∫£n (txt, md, json,...)
documents = SimpleDirectoryReader("../crawl").load_data()  # ch·ª©a t√†i li·ªáu UET

# 2. Kh·ªüi t·∫°o model embedding (E5 base)
embed_model = HuggingFaceEmbedding(model_name="intfloat/multilingual-e5-base")

# 3. Kh·ªüi t·∫°o FAISS vector store
dimension = 768  # E5-base vector size
faiss_index = faiss.IndexFlatL2(dimension)
vector_store = FaissVectorStore(faiss_index=faiss_index)

# 4. T·∫°o service context
Settings.embed_model = embed_model

# 5. Chia vƒÉn b·∫£n th√†nh ƒëo·∫°n ng·∫Øn (chunk)
node_parser = SentenceSplitter(chunk_size=256, chunk_overlap=40)

# 6. T·∫°o index vector
index = VectorStoreIndex.from_documents(
    documents,
    vector_store=vector_store,
    transformations=[node_parser]
)

# 7. T·∫°o c√¥ng c·ª• truy v·∫•n (query engine)
query_engine = index.as_query_engine(similarity_top_k=3)

# 8. Chat v·ªõi RAG
while True:
    query = input("üë§ B·∫°n h·ªèi: ")
    if query.lower() in ["exit", "quit"]:
        break
    response = query_engine.query(query)
    print("ü§ñ Tr·∫£ l·ªùi:", response)