Traceback (most recent call last):
  File "D:\LLMs\End-to-end-NLP-System-Building-ASM\scripts\main.py", line 66, in <module>
    main(model_index=1)
  File "D:\LLMs\End-to-end-NLP-System-Building-ASM\scripts\main.py", line 35, in main
    llm_service = LLMService(
  File "D:\LLMs\End-to-end-NLP-System-Building-ASM\scripts\llm_services.py", line 31, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
  File "C:\Users\LTLS\anaconda3\envs\chatbot\lib\site-packages\transformers\models\auto\auto_factory.py", line 573, in from_pretrained
    return model_class.from_pretrained(
  File "C:\Users\LTLS\anaconda3\envs\chatbot\lib\site-packages\transformers\modeling_utils.py", line 269, in _wrapper
    return func(*args, **kwargs)
  File "C:\Users\LTLS\anaconda3\envs\chatbot\lib\site-packages\transformers\modeling_utils.py", line 4434, in from_pretrained
    ) = cls._load_pretrained_model(
  File "C:\Users\LTLS\anaconda3\envs\chatbot\lib\site-packages\transformers\modeling_utils.py", line 4823, in _load_pretrained_model
    caching_allocator_warmup(model_to_load, expanded_device_map, factor=2 if hf_quantizer is None else 4)
  File "C:\Users\LTLS\anaconda3\envs\chatbot\lib\site-packages\transformers\modeling_utils.py", line 5830, in caching_allocator_warmup
    _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 4.00 GiB of which 3.33 GiB is free. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
